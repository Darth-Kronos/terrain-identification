{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from dataloader import train_dataloader, val_dataloader, SubjectDataset\n",
    "from model import BRNN, OneDConvNet\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'mps'\n",
    "input_size = 6\n",
    "sequence_length = 335413\n",
    "num_layers = 2\n",
    "hidden_size = 125\n",
    "num_classes = 4\n",
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def train_step(X, y, model, optimizer, criterion):\n",
    "\n",
    "    y_pred = model(X)\n",
    "    predicted_classes = torch.argmax(y_pred.detach(), dim=1)\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    corrects = accuracy_fn(y, y_pred.argmax(dim=1))\n",
    "\n",
    "    return loss.item(), corrects\n",
    "\n",
    "def val_step(X, y, model, criterion):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        predicted_classes = torch.argmax(y_pred.detach(), dim=1)\n",
    "        loss = criterion(y_pred, y)\n",
    "        corrects = accuracy_fn(y, y_pred.argmax(dim=1))\n",
    "\n",
    "    return loss.item(), corrects, predicted_classes.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "stats = {\n",
    "  \"acc_x\": {\n",
    "    \"min\": -39.13261,\n",
    "    \"max\": 39.26\n",
    "  },\n",
    "  \"acc_y\": {\n",
    "    \"min\": -38.92137,\n",
    "    \"max\": 39.49\n",
    "  },\n",
    "  \"acc_z\": {\n",
    "    \"min\": -31.50025,\n",
    "    \"max\": 38.1886\n",
    "  },\n",
    "  \"gyro_x\": {\n",
    "    \"min\": -11.62605,\n",
    "    \"max\": 10.72668\n",
    "  },\n",
    "  \"gyro_y\": {\n",
    "    \"min\": -12.19817,\n",
    "    \"max\": 10.93212\n",
    "  },\n",
    "  \"gyro_z\": {\n",
    "    \"min\": -6.345545,\n",
    "    \"max\": 8.093803\n",
    "  }\n",
    "}\n",
    "min = np.array([v[\"min\"] for k, v in stats.items()])\n",
    "max = np.array([v[\"max\"] for k, v in stats.items()])\n",
    "\n",
    "min = torch.from_numpy(min).float()\n",
    "min = torch.unsqueeze((torch.unsqueeze(min, 0)), -1)\n",
    "min = min.to(device)\n",
    "max = torch.from_numpy(max).float().to(device)\n",
    "max = torch.unsqueeze((torch.unsqueeze(max, 0)), -1)\n",
    "max = max.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneDConvNet(input_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to checkpoint_model_run_1.pth\n",
      "Epoch: 0 | train_loss 0.2796646486503591 | train_acc: 88.9086238374193 | val_loss: 0.5636443219565112 | val_acc: 78.36601370073892\n",
      "Epoch: 1 | train_loss 0.11391738437208988 | train_acc: 96.05295738520927 | val_loss: 0.8632429819125896 | val_acc: 70.87262033045977\n",
      "Epoch 00003: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch: 2 | train_loss 0.09180841475448581 | train_acc: 96.82803470246964 | val_loss: 0.6867567057582153 | val_acc: 76.3003194273399\n",
      "Epoch: 3 | train_loss 0.06924691294471763 | train_acc: 97.68180490654206 | val_loss: 0.6627791451568577 | val_acc: 83.64506362889983\n",
      "Epoch 00005: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch: 4 | train_loss 0.06357282886343504 | train_acc: 97.84090139509684 | val_loss: 0.7033233455457222 | val_acc: 80.25178956280787\n",
      "Epoch: 5 | train_loss 0.05335890559473792 | train_acc: 98.21773485823287 | val_loss: 0.7109544112764555 | val_acc: 80.38648783866995\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch: 6 | train_loss 0.04955337859684387 | train_acc: 98.3963017517721 | val_loss: 0.7492511372199688 | val_acc: 83.25491969417077\n",
      "Epoch: 7 | train_loss 0.04400735871062681 | train_acc: 98.60380970924196 | val_loss: 0.7748410470954988 | val_acc: 83.90660278119869\n",
      "Epoch 00009: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch: 8 | train_loss 0.04234354836401565 | train_acc: 98.63837673258386 | val_loss: 0.769593362910566 | val_acc: 84.94859015804597\n",
      "Epoch: 9 | train_loss 0.03884108320010588 | train_acc: 98.78309968847353 | val_loss: 0.803188514007234 | val_acc: 84.96398424671592\n",
      "Epoch 00011: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch: 10 | train_loss 0.03845396175773124 | train_acc: 98.80865459501558 | val_loss: 0.7669971890148075 | val_acc: 83.23166820607554\n",
      "Epoch: 11 | train_loss 0.03795127416206553 | train_acc: 98.84768358616641 | val_loss: 0.8187107036596742 | val_acc: 83.73678674055829\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch: 12 | train_loss 0.0365798612059213 | train_acc: 98.8877531152648 | val_loss: 0.7958675856627095 | val_acc: 83.7475305316092\n",
      "Epoch: 13 | train_loss 0.035912205303389566 | train_acc: 98.89798213237617 | val_loss: 0.8238118013431286 | val_acc: 84.24751770320198\n",
      "Epoch 00015: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch: 14 | train_loss 0.03531243145414594 | train_acc: 98.95711643302181 | val_loss: 0.8008312114096922 | val_acc: 83.76067964901478\n",
      "Epoch: 15 | train_loss 0.03557039977429456 | train_acc: 98.94616433021807 | val_loss: 0.8340101303725407 | val_acc: 84.57640599343186\n",
      "Epoch 00017: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch: 16 | train_loss 0.03572714036421972 | train_acc: 98.92150887173236 | val_loss: 0.8300855036867761 | val_acc: 84.49590773809523\n",
      "Epoch: 17 | train_loss 0.03446736407472519 | train_acc: 98.98023753894081 | val_loss: 0.830279390060011 | val_acc: 84.45998819786534\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch: 18 | train_loss 0.03426014467844641 | train_acc: 98.97708065262539 | val_loss: 0.8401421058880186 | val_acc: 84.58201842159278\n",
      "Epoch: 19 | train_loss 0.03543729021920903 | train_acc: 98.927505192108 | val_loss: 0.8355469606474213 | val_acc: 84.49478525246305\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Train for \"n\" number of iterations\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    for iteration, (X, y) in enumerate(train_dataloader):\n",
    "\n",
    "        X = X.float().to(device)\n",
    "        # Normalize\n",
    "        X = (X - min) / (max - min)\n",
    "\n",
    "        y = y.view(X.size(0)).to(device)\n",
    "\n",
    "        loss, corrects = train_step(X, y, model, optimizer, criterion)\n",
    "\n",
    "        # Running metrics\n",
    "        running_loss = running_loss + loss \n",
    "        running_acc = running_acc + corrects\n",
    "\n",
    "        \n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    train_acc = running_acc / len(train_dataloader)\n",
    "\n",
    "    # Validate\n",
    "    running_val_loss = 0.\n",
    "    running_val_acc = 0.\n",
    "    for step, (X, y) in enumerate(val_dataloader):\n",
    "\n",
    "        X = X.float().to(device)\n",
    "        X = (X - min) / (max - min)\n",
    "\n",
    "        y = y.view(X.size(0)).to(device)\n",
    "\n",
    "        loss, corrects, predicted_classes = val_step(X, y, model, criterion)\n",
    "        # Running metrics\n",
    "        running_val_loss = running_val_loss + loss\n",
    "        running_val_acc = running_val_acc + corrects\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataloader)\n",
    "    val_acc = running_val_acc / len(val_dataloader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        # Checkpoint model\n",
    "        path = \"checkpoint_model_run_1.pth\"\n",
    "        print(f\"Saving model to {path}\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "    print(f\"Epoch: {epoch} | train_loss {train_loss} | train_acc: {train_acc} | val_loss: {val_loss} | val_acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneDConvNet(\n",
       "  (conv1): Conv1d(6, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (norm1): LayerNorm((3840,), eps=1e-05, elementwise_affine=True)\n",
       "  (pool1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (norm2): LayerNorm((3840,), eps=1e-05, elementwise_affine=True)\n",
       "  (pool2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (norm3): LayerNorm((3840,), eps=1e-05, elementwise_affine=True)\n",
       "  (pool3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (conv4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (norm4): LayerNorm((3840,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (dropout5): Dropout(p=0.4, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "FILENAME_TEMPLATE = \"subject_{}_{}__y.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/Users/purushothamanyadav/Documents/NCSU/Spring23/NN/Project/ProjC/terrain-identification/predictions/C3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"./data/TestData/window_3\"\n",
    "split_ids = [\"009_01\", \"010_01\", \"011_01\", \"012_01\"]\n",
    "batch_size = 128\n",
    "model.eval()\n",
    "for id in split_ids:\n",
    "\n",
    "    test_dataset_1sec = SubjectDataset(\n",
    "        test_data_path, \n",
    "        [id]\n",
    "    )\n",
    "    test_dataloader_1sec = DataLoader(test_dataset_1sec, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    output = []\n",
    "\n",
    "    for (X_1sec, y_1sec) in test_dataloader_1sec:\n",
    "\n",
    "        X_1sec = X_1sec.float().to(device)\n",
    "        X_1sec = (X_1sec - min) / (max - min)\n",
    "\n",
    "       \n",
    "\n",
    "        y_pred = model(X_1sec)\n",
    "        \n",
    "\n",
    "       \n",
    "        predicted_classes = torch.argmax(y_pred, dim=1).detach().cpu().numpy()\n",
    "\n",
    "        output.append(predicted_classes)\n",
    "\n",
    "    _output = np.concatenate(output, axis=0)\n",
    "\n",
    "    df = pd.DataFrame({\"label\": _output})\n",
    "\n",
    "    subject_id, session_id = id.split(\"_\")\n",
    "\n",
    "    filename = FILENAME_TEMPLATE.format(subject_id, session_id)\n",
    "    df.to_csv(os.path.join(save_dir, filename), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
